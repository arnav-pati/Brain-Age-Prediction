{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOi9sD4Ij1R00VMul8GI1EG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arnav-pati/Brain-Age-Prediction/blob/main/Reptile_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iQTSS3j_OG2x"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, optim, autograd\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import functional as F\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Learner(nn.Module):\n",
        "    '''\n",
        "    It stores a specific nn.Module class\n",
        "    '''\n",
        "\n",
        "    def __init__(self, net_class, *args) -> None:\n",
        "        '''\n",
        "        net_class is a class, not an instance\n",
        "        args: the parameters for net_class\n",
        "        '''\n",
        "        super(Learner, self).__init__()\n",
        "        assert net_class.__class__ == type\n",
        "\n",
        "        self.net = net_class(*args)\n",
        "        self.net_pi = net_class(*args)\n",
        "        self.learner_lr = 0.1\n",
        "        self.optimizer = optim.SGD(self.net_pi.parameters(), self.learner_lr)\n",
        "    \n",
        "    def parameters(self):\n",
        "        '''\n",
        "        ignore self.net_pi.parameters()\n",
        "        '''\n",
        "        return self.net.parameters()\n",
        "    \n",
        "    def update_pi(self):\n",
        "        for m_from, m_to in zip(self.net.modules(), self.net_pi.modules()):\n",
        "            # Check again for the model\n",
        "            if isinstance(m_to, nn.Linear) or isinstance(m_to, nn.Conv2d) or isinstance(m_to, nn.BatchNorm2d):\n",
        "                m_to.weight.data = m_from.weight.data.clone()\n",
        "                if m_to.bias is not None:\n",
        "                    m_to.bias.data = m_from.bias.data.clone()\n",
        "    \n",
        "    def forward(self, support_x, support_y, query_x, query_y, num_updates):\n",
        "        self.update_pi()\n",
        "        for i in range(num_updates):\n",
        "            loss, pred = self.net_pi(support_x, support_y)\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "        loss, pred = self.net_pi(query_x, query_y)\n",
        "        indices = torch.argmax(pred, dim=1)\n",
        "        correct = torch.eq(indices, query_y).sum().item()\n",
        "        acc = correct / query_y.size(0)\n",
        "        \n",
        "        grads_pi = autograd.grad(loss, self.net_pi.parameters(), create_graph=True)\n",
        "        return loss, grads_pi, acc\n",
        "    \n",
        "    def net_forward(self, support_x, support_y):\n",
        "        loss, pred = self.net(support_x, support_y)\n",
        "        return loss, pred"
      ],
      "metadata": {
        "id": "CJN25DcUOmCR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MetaLearner(nn.Module):\n",
        "    def __init__(self, net_class, net_class_args, n_way, k_shot, meta_batchesz, beta, num_updates) -> None:\n",
        "        super(MetaLearner, self).__init__()\n",
        "        self.n_way = n_way\n",
        "        self.k_shot = k_shot\n",
        "        self.meta_batchesz = meta_batchesz\n",
        "        self.beta = beta\n",
        "        self.num_updates = num_updates\n",
        "\n",
        "        self.learner = Learner(net_class, *net_class_args)\n",
        "        self.optimizer = optim.Adam(self.learner.parameters(), lr=beta)\n",
        "    \n",
        "    def write_grads(self, dummy_loss, sum_grads_pi):\n",
        "        hooks = []\n",
        "        for i, v in enumerate(self.learner.parameters()):\n",
        "            h = v.register_hook(lambda grad : sum_grads_pi[i])\n",
        "            hooks.append(h)\n",
        "        \n",
        "        self.optimizer.zero_grad()\n",
        "        dummy_loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        for h in hooks:\n",
        "            h.remove()\n",
        "    \n",
        "    def forward(self, support_x, support_y, query_x, query_y):\n",
        "        "
      ],
      "metadata": {
        "id": "Ho8tgFrBsi5V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}